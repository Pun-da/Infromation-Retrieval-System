{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "JTj03wGlnnD9",
   "metadata": {
    "id": "JTj03wGlnnD9"
   },
   "source": [
    "# 1. Collecting data set and Importing necessary libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VLZ8t-vCn7VP",
   "metadata": {
    "id": "VLZ8t-vCn7VP"
   },
   "source": [
    "#### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b6805a",
   "metadata": {
    "id": "76b6805a"
   },
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rXn4RjIiNikm",
   "metadata": {
    "id": "rXn4RjIiNikm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zephyr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/zephyr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/zephyr/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WriA7y8Sn_xO",
   "metadata": {
    "id": "WriA7y8Sn_xO"
   },
   "source": [
    "#### 1.2 Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2603ecda",
   "metadata": {
    "id": "2603ecda"
   },
   "outputs": [],
   "source": [
    "#Reading input text files\n",
    "text = []\n",
    "names = []\n",
    "for root, dir, files in os.walk('shakespeares-works_TXT_FolgerShakespeare'):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), 'r') as rd:\n",
    "            text.append(rd.read())     #appending all the documents to text list\n",
    "            names.append(file)        #appending all the document names to name list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02Pj8YTNoE2S",
   "metadata": {
    "id": "02Pj8YTNoE2S"
   },
   "source": [
    "# 2. Removal of punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fcb297",
   "metadata": {
    "id": "76fcb297"
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "# tokenise the document\n",
    "def tokenize(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    token_words= [word for word in words if word.isalnum()]     #takes only the charecters which are either numbers or alphabets\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b284948",
   "metadata": {
    "id": "8b284948"
   },
   "outputs": [],
   "source": [
    "# remove stop words from tokens\n",
    "stopwords = stopwords.words('english')\n",
    "def stopwords_clr(sentence):\n",
    "    tokens_clr= [token for token in sentence if token.lower() not in stopwords] #takes only the words which are not in stopwords\n",
    "    return tokens_clr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4v98f8ioUht",
   "metadata": {
    "id": "p4v98f8ioUht"
   },
   "source": [
    "# 3. Normalization using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051bb44b",
   "metadata": {
    "id": "051bb44b"
   },
   "outputs": [],
   "source": [
    "#stemming the words to root form\n",
    "stem = PorterStemmer()\n",
    "\n",
    "def stem_tokens(sentence):\n",
    "    tokens_stem = []\n",
    "    for token in sentence:\n",
    "        tokens_stem.append(stem.stem(token))     #stems the token and appends into tokens_stem list\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DYnu1zvHoq-n",
   "metadata": {
    "id": "DYnu1zvHoq-n"
   },
   "source": [
    "# 4. Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "-V_jGbRWHGqh",
   "metadata": {
    "id": "-V_jGbRWHGqh"
   },
   "outputs": [],
   "source": [
    "#tokenizes 'cont', removes stopwords and stems 'cont' \n",
    "def preprocess(cont):\n",
    "    return \" \".join(stopwords_clr(stem_tokens(stopwords_clr(tokenize(cont)))))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "L553rLkxHGqh",
   "metadata": {
    "id": "L553rLkxHGqh"
   },
   "outputs": [],
   "source": [
    "processed_data = []    #This contains the pre-processed data of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "BEaCdwWYHGqh",
   "metadata": {
    "id": "BEaCdwWYHGqh"
   },
   "outputs": [],
   "source": [
    "for i in range(len(text)):                       #goes through all the documents in text list\n",
    "  processed_data.append(preprocess(text[i]))     #appends the preprocessed document to preprocessed_data list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feba98c",
   "metadata": {
    "id": "3feba98c"
   },
   "source": [
    "# 5. Construct inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e5a308",
   "metadata": {
    "id": "d6e5a308"
   },
   "outputs": [],
   "source": [
    "inv_index = {}      #creating inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d60460f",
   "metadata": {
    "id": "0d60460f"
   },
   "outputs": [],
   "source": [
    "#Indexing the inputted document\n",
    "def indexing(document, index):\n",
    "    words = nltk.word_tokenize(document)          #tokenizes the document\n",
    "    for word in words:                          \n",
    "        if(inv_index.get(word) is None):          #check whether word is there in inv_index or not\n",
    "            inv_index[word] = [index]               \n",
    "        elif not index in inv_index.get(word):     \n",
    "            inv_index.get(word).append(index)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9d9be9a",
   "metadata": {
    "id": "c9d9be9a"
   },
   "outputs": [],
   "source": [
    "for x in range(len(processed_data)):\n",
    "    indexing(processed_data[x], x)          #indexing the preprocessd data of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "SNjSGfnEOsXN",
   "metadata": {
    "id": "SNjSGfnEOsXN"
   },
   "outputs": [],
   "source": [
    "keys = list(inv_index.keys())       #Keys contains a list of all terms in the dictionary\n",
    "postings = list(inv_index.values())    #Postings contain the posting list of all terms in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "su_LuKmoVKlB",
   "metadata": {
    "id": "su_LuKmoVKlB"
   },
   "source": [
    "# 6. Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "A8H_59jAR8uu",
   "metadata": {
    "id": "A8H_59jAR8uu"
   },
   "outputs": [],
   "source": [
    "#To find Levenshtein distance of two terms\n",
    "def levenshtein_distance(term1, term2):\n",
    "    term1 = term1.lower()\n",
    "    term2 = term2.lower()\n",
    "    dyn_mat = [[0 for x in range(len(term2) + 1)] for x in range(len(term1) + 1)]\n",
    "\n",
    "    for x in range(len(term1) + 1):\n",
    "        dyn_mat[x][0] = x\n",
    "    for y in range(len(term2) + 1):\n",
    "        dyn_mat[0][y] = y\n",
    "\n",
    "    for x in range(1, len(term1) + 1):\n",
    "        for y in range(1, len(term2) + 1):\n",
    "            if term1[x - 1] == term2[y - 1]:\n",
    "                dyn_mat[x][y] = min(\n",
    "                    dyn_mat[x - 1][y] + 1,\n",
    "                    dyn_mat[x - 1][y - 1],\n",
    "                    dyn_mat[x][y - 1] + 1\n",
    "                )\n",
    "            else:\n",
    "                dyn_mat[x][y] = min(\n",
    "                    dyn_mat[x - 1][y] + 1,\n",
    "                    dyn_mat[x - 1][y - 1] + 1,\n",
    "                    dyn_mat[x][y - 1] + 1\n",
    "                )\n",
    "\n",
    "    return dyn_mat[len(term1)][len(term2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3P5nizrnTDj-",
   "metadata": {
    "id": "3P5nizrnTDj-"
   },
   "outputs": [],
   "source": [
    "#To find the nearest word in the dictionary to a misspelled word\n",
    "def nearest_word(word):\n",
    "  min = 100\n",
    "  near = ''\n",
    "  for key in keys:\n",
    "    leven = levenshtein_distance(word, key)     #finding the levenshtein distance of word and key\n",
    "    if leven == 0:                              \n",
    "      min = leven                               \n",
    "      near = key\n",
    "      return near\n",
    "    if leven < min:\n",
    "      near = key\n",
    "      min = leven\n",
    "  return near"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "JzpVXkOsotpq",
   "metadata": {
    "id": "JzpVXkOsotpq"
   },
   "outputs": [],
   "source": [
    "def gen_posting(term, inv_index):        \n",
    "  near = nearest_word(term)\n",
    "  posting = inv_index[near]\n",
    "  return posting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZWDuvwWloAa_",
   "metadata": {
    "id": "ZWDuvwWloAa_"
   },
   "source": [
    "# 7. Wildcard Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b8q5YKLxGB",
   "metadata": {
    "id": "a4b8q5YKLxGB"
   },
   "outputs": [],
   "source": [
    "def rotate(s, n):\n",
    "    return s[n:] + s[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "xLBPiqQQOZUj",
   "metadata": {
    "id": "xLBPiqQQOZUj"
   },
   "outputs": [],
   "source": [
    "def bit_and(X, Y):\n",
    "    return set(X).intersection(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "BOgAxcsEfed1",
   "metadata": {
    "id": "BOgAxcsEfed1"
   },
   "outputs": [],
   "source": [
    "def bit_or(X, Y):\n",
    "    return set(X).union(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lydl1LXWPY5-",
   "metadata": {
    "id": "lydl1LXWPY5-"
   },
   "outputs": [],
   "source": [
    "#Generating all permuterms for a term\n",
    "def gen_perm(keys, per_index):\n",
    "  for key in keys:\n",
    "    okey = key + \"$\"\n",
    "    for i in range(len(okey),0,-1):\n",
    "      rot = rotate(okey, i)\n",
    "      per_index[rot] = key\n",
    "  return per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "C6URF2nOL2dv",
   "metadata": {
    "id": "C6URF2nOL2dv"
   },
   "outputs": [],
   "source": [
    "#Find all appropriate permuterms and original terms\n",
    "def find_perm(term, prefix):\n",
    "    req_terms = []\n",
    "    for key in term.keys():\n",
    "        if key.startswith(prefix):\n",
    "            req_terms.append(term[key])\n",
    "    return req_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "LTHKrNvyMhBh",
   "metadata": {
    "id": "LTHKrNvyMhBh"
   },
   "outputs": [],
   "source": [
    "#For cases 1, 2 and 3\n",
    "def processQuery1(query, per_index):    \n",
    "    req_terms = find_perm(per_index, query)\n",
    "    #print(req_terms)\n",
    "\n",
    "    post_ID = []\n",
    "    for term in req_terms:\n",
    "        post_ID.append(inv_index[term])\n",
    "    #print(post_ID)\n",
    "\n",
    "    coll = []\n",
    "    for x in post_ID:\n",
    "        for y in x:\n",
    "            coll.append(y)   \n",
    "\n",
    "    coll = [int(x) for x in coll]\n",
    "    per = set(coll)\n",
    "    per = list(per) \n",
    "    #print(per)    \n",
    "\n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1UHqDSZwNBEU",
   "metadata": {
    "id": "1UHqDSZwNBEU"
   },
   "outputs": [],
   "source": [
    "#For case 4 (X*Y*Z)\n",
    "def processQuery2(que_part1, que_part2, per_index):\n",
    "\n",
    "  #Part 1 = Z$X\n",
    "  req_terms1 = find_perm(per_index, que_part1)\n",
    "  #print(req_terms1)\n",
    "\n",
    "  post_ID1 = []\n",
    "  for term in req_terms1:\n",
    "      post_ID1.append(inv_index[term])\n",
    "  #print(post_ID1)\n",
    "\n",
    "  coll1 = []\n",
    "  for x in post_ID1:\n",
    "      for y in x:\n",
    "          coll1.append(y) \n",
    "  #print(coll1)  \n",
    "\n",
    "  #Part 2 = Y\n",
    "  req_terms2 = find_perm(per_index, que_part2)\n",
    "  #print(req_terms2)\n",
    "\n",
    "  post_ID2 = []\n",
    "  for term in req_terms2:\n",
    "      post_ID2.append(inv_index[term])\n",
    "  #print(post_ID2)\n",
    "\n",
    "  coll2 = []\n",
    "  for x in post_ID2:\n",
    "      for y in x:\n",
    "          coll2.append(y) \n",
    "  #print(coll2)  \n",
    "\n",
    "  #Intersecting the two posting lists obtained above\n",
    "\n",
    "  coll1 = [int(x) for x in coll1]\n",
    "  coll2 = [int(x) for x in coll2]\n",
    "\n",
    "  coll_final = bit_and(coll1, coll2)\n",
    "  per_final = set(coll_final)\n",
    "  per_final = list(per_final)\n",
    "  #print(per_final)\n",
    "\n",
    "  return per_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "p-1BHY1XLOcE",
   "metadata": {
    "id": "p-1BHY1XLOcE"
   },
   "outputs": [],
   "source": [
    "#Decide case and process the wildcard query accordingly\n",
    "def wildcard_process(query):\n",
    "  out = []\n",
    "  per_index = []\n",
    "  comps = query.split('*')\n",
    "  case = 0\n",
    "\n",
    "  if len(comps) == 3:\n",
    "    case = 4\n",
    "  elif comps[1] == '':\n",
    "    case = 1\n",
    "  elif comps[0] == '':\n",
    "    case = 2\n",
    "  elif comps[0] != '' and comps[1] != '':\n",
    "    case = 3\n",
    "\n",
    "  per_index = {}\n",
    "  per_index = gen_perm(keys, per_index)\n",
    "\n",
    "  if case == 1:\n",
    "    query = \"$\" + comps[0]\n",
    "  elif case == 2:\n",
    "    query = comps[1] + \"$\"\n",
    "  elif case == 3:\n",
    "    query = comps[1] + \"$\" + comps[0]\n",
    "  elif case == 4:\n",
    "    que_part1 = comps[2] + \"$\" + comps[0]\n",
    "    que_part2 = comps[1]\n",
    "    #print(que_part1, que_part2)\n",
    "\n",
    "  if case != 4:\n",
    "    out = processQuery1(query, per_index)\n",
    "  elif case == 4:\n",
    "    out = processQuery2(que_part1, que_part2, per_index)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Qvtu23B2Eul",
   "metadata": {
    "id": "5Qvtu23B2Eul"
   },
   "source": [
    "# 8. Boolean query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "pDwBCqAH2FUy",
   "metadata": {
    "id": "pDwBCqAH2FUy"
   },
   "outputs": [],
   "source": [
    "#Processing a boolean query and finding the appropriate documents\n",
    "def boolean_query(query, inv_index):\n",
    "  terms = query.split(' ')\n",
    "  bool_words = []\n",
    "  diff_words = []\n",
    "\n",
    "  for term in terms:\n",
    "    if term.lower() != 'and' and term.lower() != 'or' and term.lower() != 'not':\n",
    "      diff_words.append(term)\n",
    "    else:\n",
    "      bool_words.append(term)\n",
    "  \n",
    "  #print(bool_words, diff_words)\n",
    "  \n",
    "  posting_term = []\n",
    "  posting_comb = []\n",
    "\n",
    "  for term in diff_words:\n",
    "    if '*' in term:\n",
    "      posting_term = wildcard_process(term)\n",
    "      posting_comb.append(posting_term)\n",
    "    else:\n",
    "      posting_term = gen_posting(term, inv_index)\n",
    "      posting_comb.append(posting_term)\n",
    "\n",
    "  #print(posting_comb)\n",
    "\n",
    "\n",
    "  i = 0\n",
    "  x = 0\n",
    "  z = len(bool_words)\n",
    "    \n",
    "  while i < z:\n",
    "    \n",
    "    if bool_words[x] == 'not':\n",
    "      all_docs = set(list(range(len(processed_data))))\n",
    "      res = list(all_docs - set(posting_comb[x]))\n",
    "      posting_comb.remove(posting_comb[x])\n",
    "      posting_comb.insert(x, res)\n",
    "      bool_words.remove(bool_words[x])\n",
    "      i = i + 1\n",
    "    \n",
    "    elif bool_words[x] == 'and':\n",
    "        if (x + 1) < len(bool_words) and bool_words[x + 1] == 'not':\n",
    "            all_docs = set(list(range(len(processed_data))))\n",
    "            res = list(all_docs - set(posting_comb[x + 1]))\n",
    "            bool_words.remove(bool_words[x + 1])\n",
    "            i = i + 1\n",
    "        else:\n",
    "            res = posting_comb[x + 1]\n",
    "        intersection = list(set(posting_comb[x]).intersection(res))\n",
    "        posting_comb.remove(posting_comb[x])\n",
    "        posting_comb.remove(posting_comb[x])\n",
    "        posting_comb.insert(x, intersection)\n",
    "        bool_words.remove(bool_words[x])\n",
    "        i = i + 1\n",
    "        \n",
    "    elif bool_words[x] == 'or':\n",
    "        x = x + 1\n",
    "        i = i + 1\n",
    "        \n",
    "  #print(posting_comb)\n",
    "  #print(bool_words)\n",
    "    \n",
    "  i = 0      \n",
    "  while i < len(bool_words):\n",
    "    union = posting_comb[0] + list(set(posting_comb[1]) - set(posting_comb[0]))\n",
    "    #print(union)\n",
    "    posting_comb.remove(posting_comb[0])\n",
    "    posting_comb.remove(posting_comb[0])\n",
    "    posting_comb.insert(0, union)\n",
    "    i = i + 1\n",
    "         \n",
    "      \n",
    "  #print(posting_comb)\n",
    "  return posting_comb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "WYIgVBpI9Lb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYIgVBpI9Lb6",
    "outputId": "7830cfa5-ab30-4678-cf1e-286d0aea8094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antony\n"
     ]
    }
   ],
   "source": [
    "inp_query = input()\n",
    "\n",
    "out = []\n",
    "out = boolean_query(inp_query, inv_index)\n",
    "output = open(\"OUTPUT_Documents.txt\",\"w\")\n",
    "\n",
    "for x in out:\n",
    "  output.write(names[x] + '\\n')\n",
    "\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IR_Assignment_1_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
